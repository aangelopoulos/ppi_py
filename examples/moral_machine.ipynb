{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moral Machine Experiment with LLMs and PPI\n",
    "\n",
    "Prior work shows that LLMs can make serious errors when predicting human behavior. How can we draw valid, precise, and cost-effective inferences from LLMs on causal effects (and other parameters)? Broska, Howes, and van Loon (1) propose using human subjects as a gold standard to adjust inaccurate estimates based on LLMs with PPI. \n",
    "\n",
    "The authors add two extensions for PPI implemented in the PPI Python library. \n",
    "- The PPI correlation $\\tilde \\rho$ measures the *interchangeability* of gold-standard data (observed human behavior) and predictions (LLM predicted behavior). Values closer to 1 mean that an estimate based on predictions is close to an estimate obtained from observations.\n",
    "- If predictions and observations are not interchangeable (PPI correlation<1), their power analysis resolves a trade-off. Researchers can optimally choose between recruiting informative but costly human subjects or obtaining less informative but cheap predictions of human behavior.\n",
    "\n",
    "In the Moral Machine Experiment, a sudden brake failure in an autonomous vehicle forced a decision between harming passengers or pedestrians. LLMs were prompted to predict which group the survey respondents chose to spare. \n",
    "\n",
    "1. Broska D, Howes M, van Loon A. The Mixed Subjects Design: Treating Large Language Models as (Potentially) Informative Observations [Internet]. OSF; 2024 [cited 2024 Aug 29]. Available from: https://osf.io/j3bnt\n",
    "2. Awad E, Dsouza S, Kim R, Schulz J, Henrich J, Shariff A, et al. The Moral Machine experiment. Nature. 2018 Nov;563(7729):59–64. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.11.4 (v3.11.4:d2340ef257, Jun  6 2023, 19:15:51) [Clang 13.0.0 (clang-1300.0.29.30)]'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os, sys\n",
    "#sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "\n",
    "# setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "from ppi_py import ppi_ols_ci, classical_ols_ci, ppi_ols_pointestimate\n",
    "\n",
    "df = pd.read_csv(\"5_SurveySampleLLM_ppipy.csv.gz\")\n",
    "\n",
    "Covs = ['PedPed', 'Barrier', 'CrossingSignal', 'NumberOfCharacters',\n",
    "        'DiffNumberOFCharacters', 'LeftHand', 'Man', 'Woman', 'Pregnant',\n",
    "        'Stroller', 'OldMan', 'OldWoman', 'Boy', 'Girl', 'Homeless',\n",
    "        'LargeWoman', 'LargeMan', 'Criminal', 'MaleExecutive',\n",
    "        'FemaleExecutive', 'FemaleAthlete', 'MaleAthlete', 'FemaleDoctor',\n",
    "        'MaleDoctor', 'Dog', 'Cat', \n",
    "        'Intervention'\n",
    "        ]\n",
    "\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirical application \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to calculate weights for conjoint experiment\n",
    "def CalcTheoreticalInt(r):\n",
    "    # this function is applied to each row (r)\n",
    "    if r[\"Intervention\"]==0:\n",
    "        if r[\"Barrier\"]==0:\n",
    "            if r[\"PedPed\"]==1: p = 0.48\n",
    "            else: p = 0.32\n",
    "            \n",
    "            if r[\"CrossingSignal\"]==0:   p = p * 0.48\n",
    "            elif r[\"CrossingSignal\"]==1: p = p * 0.2\n",
    "            else: p = p * 0.32\n",
    "        else: p = 0.2\n",
    "\n",
    "    else: \n",
    "        if r[\"Barrier\"]==0:\n",
    "            if r[\"PedPed\"]==1: \n",
    "                p = 0.48\n",
    "                if r[\"CrossingSignal\"]==0: p = p * 0.48\n",
    "                elif r[\"CrossingSignal\"]==1: p = p * 0.32\n",
    "                else: p = p * 0.2\n",
    "            else: \n",
    "                p = 0.2\n",
    "                if r[\"CrossingSignal\"]==0: p = p * 0.48\n",
    "                elif r[\"CrossingSignal\"]==1: p = p * 0.2\n",
    "                else: p = p * 0.32\n",
    "        else: p = 0.32  \n",
    "    \n",
    "    return(p)  \n",
    "        \n",
    "def calcWeightsTheoretical(profiles):\n",
    "    \n",
    "    p = profiles.apply(CalcTheoreticalInt, axis=1)\n",
    "\n",
    "    weight = 1/p \n",
    "\n",
    "    return(weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate amce with ppi \n",
    "def compute_amce_ppi(n_data, N_data, x, y, alpha=0.05):\n",
    "\n",
    "    # specify regression for swerve or stay in lane\n",
    "    if x==\"Intervention\":\n",
    "        \n",
    "        # calculate weights\n",
    "        n_data.loc[:,\"weights\"] = calcWeightsTheoretical(n_data)\n",
    "        N_data.loc[:,\"weights\"] = calcWeightsTheoretical(N_data)\n",
    "    \n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data.dropna(subset=y)\n",
    "        N_dd = N_data.dropna(subset=y)\n",
    "\n",
    "        # if X=1 characters die if AV serves, if X=0 characters if AV stays\n",
    "        n_X = n_dd[\"Intervention\"]               \n",
    "        N_X = N_dd[\"Intervention\"]\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "\n",
    "    # specify regression for relationship to vehicle\n",
    "    if x==\"Barrier\":\n",
    "\n",
    "        # consider only dilemmas without legality and only pedestrians vs passengers\n",
    "        n_data_sub = n_data.loc[(n_data[\"CrossingSignal\"]==0) & (n_data[\"PedPed\"]==0), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"CrossingSignal\"]==0) & (N_data[\"PedPed\"]==0), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "        \n",
    "        # if X=1 passengers die and if X=0 pedestrians die\n",
    "        n_X = n_dd[\"Barrier\"]\n",
    "        N_X = N_dd[\"Barrier\"]\n",
    "\n",
    "        # recode to estimate the preference for pedestrians over passengers \n",
    "        n_X = 1 - n_X\n",
    "        N_X = 1 - N_X\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "    \n",
    "\n",
    "    # specify regression for legality\n",
    "    if x==\"CrossingSignal\": \n",
    "        \n",
    "        # consider dilemmas with legality and only pedestrians vs pedestrians\n",
    "        n_data_sub = n_data.loc[(n_data[\"CrossingSignal\"]!=0) & (n_data[\"PedPed\"]==1), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"CrossingSignal\"]!=0) & (N_data[\"PedPed\"]==1), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # if X=1 pedestrians cross on a green light, if X=2 pedestrians cross on a red light \n",
    "        n_X = n_dd[\"CrossingSignal\"]\n",
    "        N_X = N_dd[\"CrossingSignal\"]\n",
    "\n",
    "        # create dummy variable to estimate preference for pedestrians that cross legally (1) vs legally (0)\n",
    "        n_X = 2 - n_X \n",
    "        N_X = 2 - N_X \n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "    \n",
    "\n",
    "\n",
    "    # Specify regressions for the remaining six attributes\n",
    "    if x==\"Utilitarian\":\n",
    "        \n",
    "        # consider dilemmas that compare 'More' versus 'Less' characters\n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Utilitarian\") & (n_data[\"ScenarioTypeStrict\"]==\"Utilitarian\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Utilitarian\") & (N_data[\"ScenarioTypeStrict\"]==\"Utilitarian\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "        \n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Utilitarian'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Utilitarian'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing more characters\n",
    "        n_X = (n_dd.loc[:,\"Utilitarian\"]==\"More\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Utilitarian\"]==\"More\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "\n",
    "    if x==\"Species\":\n",
    "        \n",
    "        # consider dilemmas that compare humans versus animals \n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Species\") & (n_data[\"ScenarioTypeStrict\"]==\"Species\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Species\") & (N_data[\"ScenarioTypeStrict\"]==\"Species\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Species'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Species'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing humans\n",
    "        n_X = (n_dd.loc[:,\"Species\"]==\"Hoomans\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Species\"]==\"Hoomans\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "    \n",
    "\n",
    "    if x==\"Gender\":\n",
    "        \n",
    "        # consider dilemmas that compare women versus men\n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Gender\") & (n_data[\"ScenarioTypeStrict\"]==\"Gender\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Gender\") & (N_data[\"ScenarioTypeStrict\"]==\"Gender\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Gender'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Gender'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing women\n",
    "        n_X = (n_dd.loc[:,\"Gender\"]==\"Female\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Gender\"]==\"Female\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "\n",
    "    if x==\"Fitness\":\n",
    "        \n",
    "        # consider dilemmas that compare fit characters versus those that are not\n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Fitness\") & (n_data[\"ScenarioTypeStrict\"]==\"Fitness\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Fitness\") & (N_data[\"ScenarioTypeStrict\"]==\"Fitness\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Fitness'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Fitness'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing fit characters\n",
    "        n_X = (n_dd.loc[:,\"Fitness\"]==\"Fit\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Fitness\"]==\"Fit\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "\n",
    "    if x==\"Age\":\n",
    "        \n",
    "        # consider dilemmas that compare younger versus older characters\n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Age\") & (n_data[\"ScenarioTypeStrict\"]==\"Age\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Age\") & (N_data[\"ScenarioTypeStrict\"]==\"Age\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Age'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Age'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing younger characters\n",
    "        n_X = (n_dd.loc[:,\"Age\"]==\"Young\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Age\"]==\"Young\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd[\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd[y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd[\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "    \n",
    "    if x==\"Social Status\":\n",
    "        \n",
    "        # consider dilemmas that compare high status versus low status characters\n",
    "        n_data_sub = n_data.loc[(n_data[\"ScenarioType\"]==\"Social Status\") & (n_data[\"ScenarioTypeStrict\"]==\"Social Status\"), :].copy()\n",
    "        N_data_sub = N_data.loc[(N_data[\"ScenarioType\"]==\"Social Status\") & (N_data[\"ScenarioTypeStrict\"]==\"Social Status\"), :].copy()\n",
    "\n",
    "        # calculate weights\n",
    "        n_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(n_data_sub)\n",
    "        N_data_sub.loc[:,\"weights\"] = calcWeightsTheoretical(N_data_sub)\n",
    "\n",
    "        # drop rows with missing values on dependent variable\n",
    "        n_dd = n_data_sub.dropna(subset=y)\n",
    "        N_dd = N_data_sub.dropna(subset=y)\n",
    "\n",
    "        # rename column to extract coefficient from result\n",
    "        n_dd = n_dd.rename(columns = {'AttributeLevel': 'Social Status'})\n",
    "        N_dd = N_dd.rename(columns = {'AttributeLevel': 'Social Status'})\n",
    "\n",
    "        # create dummy variable to estimate the preference for sparing high status characters\n",
    "        n_X = (n_dd.loc[:,\"Social Status\"]==\"High\").astype(int)\n",
    "        N_X = (N_dd.loc[:,\"Social Status\"]==\"High\").astype(int)\n",
    "\n",
    "        # add intercept\n",
    "        n_X = np.column_stack((np.ones(n_X.shape[0]), n_X))\n",
    "        N_X = np.column_stack((np.ones(N_X.shape[0]), N_X))\n",
    "\n",
    "        # gold standard data\n",
    "        n_Y_human   = n_dd.loc[:,\"Saved\"].to_numpy()    # observed outcomes\n",
    "        n_Y_silicon = n_dd.loc[:,y].to_numpy()          # predicted outcomes\n",
    "        n_weights = n_dd.loc[:,\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "        # unlabeled data\n",
    "        N_Y_silicon = N_dd[y].to_numpy()          # predicted outcomes\n",
    "        N_weights = N_dd.loc[:,\"weights\"].to_numpy()    # define weights\n",
    "\n",
    "\n",
    "    # calculate point estimate\n",
    "    pointest_ppi = ppi_ols_pointestimate(X=n_X, Y=n_Y_human, Yhat=n_Y_silicon, \n",
    "                                         X_unlabeled=N_X, Yhat_unlabeled=N_Y_silicon, \n",
    "                                         w=n_weights, w_unlabeled=N_weights)\n",
    "\n",
    "    # calculate PPI confidence intervals\n",
    "    lower_CI_ppi, upper_CI_ppi = ppi_ols_ci(X=n_X, Y=n_Y_human, Yhat=n_Y_silicon, \n",
    "                                            X_unlabeled=N_X, Yhat_unlabeled=N_Y_silicon, \n",
    "                                            w=n_weights, w_unlabeled=N_weights, alpha=alpha)\n",
    "    \n",
    "    # calculate OLS confidence intervals\n",
    "    lower_CI_ols, upper_CI_ols = classical_ols_ci(X=n_X, Y=n_Y_human, w=n_weights, alpha=alpha)\n",
    "\n",
    "    # calculate rho\n",
    "    #beta = sm.WLS(n_Y_human, n_X, weights=n_weights).fit().params\n",
    "\n",
    "    #grads, grads_hat, grads_hat_unlabeled, inv_hessian = _ols_get_stats(\n",
    "    #    pointest=beta, \n",
    "    #    X=n_X,\n",
    "    #    Y=n_Y_human,\n",
    "    #   Yhat= n_Y_silicon,\n",
    "    #    X_unlabeled=N_X,\n",
    "    #    Yhat_unlabeled=N_Y_silicon,\n",
    "    #   w=n_weights,\n",
    "    #    w_unlabeled=N_weights,\n",
    "    #    use_unlabeled=False)\n",
    "    \n",
    "    #rho_sq, var_y = _power_analysis_stats(grads, grads_hat, inv_hessian)\n",
    "\n",
    "    # create and return the output DataFrame\n",
    "    output_df = pd.DataFrame({\n",
    "        \"y\": y,                              \n",
    "        \"x\": x,                              # Predictor variable (scenario attribute)\n",
    "        \"pointest_ppi\": pointest_ppi[1],     # PPI point estimate\n",
    "        \"conf_low_ppi\": lower_CI_ppi[1],     # The lower bound of the PPI confidence interval\n",
    "        \"conf_high_ppi\": upper_CI_ppi[1],    # The upper bound of the PPI confidence interval\n",
    "        \"conf_low_ols\": lower_CI_ols[1],     # The lower bound of the OLS confidence interval\n",
    "        \"conf_high_ols\": upper_CI_ols[1],    # The upper bound of the OLS confidence interval\n",
    "        #\"rho\": np.sqrt(rho_sq[1])          # The association between predictions and outcomes\n",
    "    },index=[0])\n",
    "    \n",
    "    return output_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this particular experiment, the researchers found that GPT4's predictions of decisions to moral dilemmas are mostly not interchangeable with observed decisions from survey respondents (PPI correlation < 0.36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of respondents:  2097\n",
      "Number of decisions:  22315\n",
      "Number of NAs in observed dependent variable:  0\n",
      "Number of NAs in predicted dependent variable with GPT4 Turbo:  0\n"
     ]
    }
   ],
   "source": [
    "# basic statistics\n",
    "print(\"Number of respondents: \", len(df[\"UserID\"].unique()))\n",
    "print(\"Number of decisions: \", len(df[\"ResponseID\"].unique()))\n",
    "print(\"Number of NAs in observed dependent variable: \", df[\"Saved\"].isna().sum())\n",
    "print(\"Number of NAs in predicted dependent variable with GPT4 Turbo: \", df[\"gpt4turbo_wp_Saved\"].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  gpt4turbo_wp_Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/md/0h30crhs7xb714g8zy7dzj1m0000gn/T/ipykernel_33436/2240639833.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  n_data.loc[:,\"weights\"] = calcWeightsTheoretical(n_data)\n",
      "/var/folders/md/0h30crhs7xb714g8zy7dzj1m0000gn/T/ipykernel_33436/2240639833.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  N_data.loc[:,\"weights\"] = calcWeightsTheoretical(N_data)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>pointest_ppi</th>\n",
       "      <th>conf_low_ppi</th>\n",
       "      <th>conf_high_ppi</th>\n",
       "      <th>conf_low_ols</th>\n",
       "      <th>conf_high_ols</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Intervention</td>\n",
       "      <td>0.074720</td>\n",
       "      <td>0.018721</td>\n",
       "      <td>0.130527</td>\n",
       "      <td>0.004623</td>\n",
       "      <td>0.124348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Barrier</td>\n",
       "      <td>0.164356</td>\n",
       "      <td>0.079992</td>\n",
       "      <td>0.248722</td>\n",
       "      <td>0.080195</td>\n",
       "      <td>0.255824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Gender</td>\n",
       "      <td>0.115466</td>\n",
       "      <td>-0.011634</td>\n",
       "      <td>0.243097</td>\n",
       "      <td>0.008117</td>\n",
       "      <td>0.280942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Fitness</td>\n",
       "      <td>0.089433</td>\n",
       "      <td>-0.044907</td>\n",
       "      <td>0.223705</td>\n",
       "      <td>-0.004669</td>\n",
       "      <td>0.283155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Social Status</td>\n",
       "      <td>-0.042173</td>\n",
       "      <td>-0.495101</td>\n",
       "      <td>0.410756</td>\n",
       "      <td>-0.485565</td>\n",
       "      <td>0.401220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>CrossingSignal</td>\n",
       "      <td>0.338596</td>\n",
       "      <td>0.231966</td>\n",
       "      <td>0.444595</td>\n",
       "      <td>0.191863</td>\n",
       "      <td>0.414262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Age</td>\n",
       "      <td>0.512092</td>\n",
       "      <td>0.385869</td>\n",
       "      <td>0.637253</td>\n",
       "      <td>0.381740</td>\n",
       "      <td>0.634357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Utilitarian</td>\n",
       "      <td>0.618782</td>\n",
       "      <td>0.515252</td>\n",
       "      <td>0.725339</td>\n",
       "      <td>0.523043</td>\n",
       "      <td>0.733611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpt4turbo_wp_Saved</td>\n",
       "      <td>Species</td>\n",
       "      <td>0.656106</td>\n",
       "      <td>0.551581</td>\n",
       "      <td>0.761534</td>\n",
       "      <td>0.536525</td>\n",
       "      <td>0.747782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    y               x  pointest_ppi  conf_low_ppi  \\\n",
       "0  gpt4turbo_wp_Saved    Intervention      0.074720      0.018721   \n",
       "1  gpt4turbo_wp_Saved         Barrier      0.164356      0.079992   \n",
       "2  gpt4turbo_wp_Saved          Gender      0.115466     -0.011634   \n",
       "3  gpt4turbo_wp_Saved         Fitness      0.089433     -0.044907   \n",
       "4  gpt4turbo_wp_Saved   Social Status     -0.042173     -0.495101   \n",
       "5  gpt4turbo_wp_Saved  CrossingSignal      0.338596      0.231966   \n",
       "6  gpt4turbo_wp_Saved             Age      0.512092      0.385869   \n",
       "7  gpt4turbo_wp_Saved     Utilitarian      0.618782      0.515252   \n",
       "8  gpt4turbo_wp_Saved         Species      0.656106      0.551581   \n",
       "\n",
       "   conf_high_ppi  conf_low_ols  conf_high_ols  \n",
       "0       0.130527      0.004623       0.124348  \n",
       "1       0.248722      0.080195       0.255824  \n",
       "2       0.243097      0.008117       0.280942  \n",
       "3       0.223705     -0.004669       0.283155  \n",
       "4       0.410756     -0.485565       0.401220  \n",
       "5       0.444595      0.191863       0.414262  \n",
       "6       0.637253      0.381740       0.634357  \n",
       "7       0.725339      0.523043       0.733611  \n",
       "8       0.761534      0.536525       0.747782  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = df[\"ResponseID\"].unique()\n",
    "n = 750\n",
    "N = len(ids) - n\n",
    "random.seed(2024)\n",
    "\n",
    "n_ids = random.sample(ids.tolist(), k=n)\n",
    "N_ids = random.sample(list(set(ids) - set(n_ids)), k=N)\n",
    "\n",
    "df_human = df[ df[\"ResponseID\"].isin(n_ids) ]\n",
    "df_silicon = df [ df[\"ResponseID\"].isin(N_ids)]\n",
    "\n",
    "models = [\"gpt4turbo_wp_Saved\"]#,\"gpt4o_wp_Saved\",\"gpt35turbo0125_wp_Saved\"]\n",
    "\n",
    "results2 = pd.DataFrame()\n",
    "for model in models: \n",
    "    \n",
    "    print(\"Model: \", model)\n",
    "    results1 = pd.concat([\n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"Intervention\", y=model), \n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"Barrier\", y=model), \n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"Gender\", y=model), \n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"Fitness\", y=model), \n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"Social Status\", y=model), \n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"CrossingSignal\",y=model),\n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"Age\", y=model),\n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"Utilitarian\", y=model),\n",
    "        compute_amce_ppi(df_human, df_silicon, x=\"Species\", y=model)\n",
    "        ],ignore_index=True)\n",
    "    \n",
    "    results2 = pd.concat([results2, results1],ignore_index=True)\n",
    "    \n",
    "\n",
    "results2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Statistical precision: We confirm that adding an increasing number of LLM predictions reduces the width of confidence intervals more strongly for higher values of the PPI correlation (higher interchangeability of predicted and observed behavior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Validity: The percent of PPI confidence intervals that cover the parameter remains stable at high levels when relying on LLM predictions. In contrast, the coverage for a naive approach that pools predictions and observations remains stable only for some independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "- The silicon sampling design may or may not produce valid point estimates but this is impossible to ascertain without validation on human subjects. PPI automatically handles this validation through a statistical correction to the point estimates derived from LLM predictions.\n",
    "\n",
    "- Mixed subjects designs––with methods such as PPI––could enhance scientific productivity and reduce inequality in access to costly evidence on research questions by offering valid, precise, and cost-effective inferences on causal effects and other parameters.\n",
    "\n",
    "- If LLMs and other algorithms become more capable of predicting human behavior in the future (higher PPI correlation), the cost of obtaining precise estimates with mixed subjects designs will further decrease relative to human subjects experiments.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
